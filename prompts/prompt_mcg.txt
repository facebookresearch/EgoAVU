OBJECTIVE:
You are an AI assistant tasked with performing a high-fidelity analysis of video content.
Your role is to function strictly as an evidence extractor, not an open-world reasoner.
You must ONLY use the provided captions to identify object interactions and analyze sounds.
Every piece of information must be explicitly grounded in the source text.

INPUTS:
You will be provided with a JSON object containing the following four keys:

1. Video Caption:
   Describes the overall visual scene, including events and actions of entities other than the narrator
   (e.g., animals, other people, environmental events).

2. Image Caption:
   Describes the objects visible in the center frame.

3. Audio Caption:
   A transcript of sounds and audio events.

4. Action Narration:
   Describes the specific actions performed by the primary person.
   (#CC denotes the primary person)

TASK:
Analyze the provided captions and generate a structured multimodal context graph in JSON format
that captures multimodal relationships.

Follow the instructions below strictly.

INSTRUCTIONS:

1. Identify Interacted Objects:
   - Parse the Action Narration.
   - Identify all objects the primary person is touching, holding, using, or manipulating.
   - For each object, record the action performed.
   - Add these to the "interacted_objects" list.

2. Identify Non-Interacted (Background) Objects:
   - Extract the complete list of objects from the Image Caption.
   - Remove all objects that appear in the "interacted_objects" list.
   - The remaining objects form the "non_interacted_objects" list.

3. Identify Soundâ€“Source Associations:
   This is a STRICT evidence-based process.

   There are two types of sounds:
   - Foreground sounds (caused by actions or objects)
   - Background sounds (ambient or scene-level)

   A. Grounding Foreground Sounds:
      - For each sound in the Audio Caption:
        * Search the Action Narration for a causal action by the primary person.
        * Search the Video Caption for causal events involving other entities or the environment.
      - If a sound has NO direct grounding evidence in either caption,
        it MUST be treated as a background sound.

   B. Exclude "Unquestionable" Sounds:
      Do NOT include sounds that fall into these categories, even if grounded:
      - Mundane biological sounds (e.g., breathing, sighing, swallowing)
      - Vague ambient noise (e.g., white noise, faint hum)

   C. Sound Categorization:
      - Classify each included sound as one of:
        * Action Sound
        * Object Sound
        * Ambient Sound

   D. Empty Case Handling:
      - If no sounds pass grounding and filtering,
        output an empty list: "sounds": []

HUMAN-GENERATED EXAMPLES:
Five human-created examples are provided in <examples> demonstrating correct execution.

IMPORTANT NOTES:
- Sounds such as "giggle" should be excluded if no grounding evidence exists
  in the Video Caption or Action Narration.
- The sound source description must reference either:
  * the specific action, or
  * the object responsible for producing the sound.
- Sound categories must clearly distinguish foreground vs background sounds.

FINAL OUTPUT FORMAT:
Your entire response MUST be a single, valid JSON object.
Do NOT include any text outside the JSON.

INPUT:
<input>

